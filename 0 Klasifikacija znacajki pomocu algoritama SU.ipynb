{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import random\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'EXTRACTED_FEATURES\\\\'\n",
    "LABEL = 'DenseNet_FE'\n",
    "MODELI = 'MODELI\\\\0_only_top\\\\'\n",
    "LR = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = CSV_PATH + \"\\\\\" + LABEL\n",
    "csv_file_train = csv_file_path + \"_train.csv\"\n",
    "csv_file_test = csv_file_path + \"_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Učitavnje podatak za treniranje i testiranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "podaci_train = pd.read_csv(csv_file_train,  header=None)\n",
    "podaci_test = pd.read_csv(csv_file_test,  header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutiraj podatke iz skupa za treniranje\n",
    "podaci_train = podaci_train.sample(frac = 1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nez_atr = podaci_train.columns[1:]\n",
    "zav_atr = podaci_train.columns[0]\n",
    "\n",
    "X_train = podaci_train[nez_atr]\n",
    "y_train = podaci_train[zav_atr]\n",
    "X_test = podaci_test[nez_atr]\n",
    "y_test = podaci_test[zav_atr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1918, 1024)\n",
      "Test: (477, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: {}\".format(X_train.shape))\n",
    "print(\"Test: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluacija_train_test(klasifikator, X_test, y_test, X_train, y_train):\n",
    "#     y_pred_test = klasifikator.predict(X_test)\n",
    "#     y_train_test = klasifikator.predict(X_train)\n",
    "#     print(\"Točnost na skupu za testiranje: {:.2f}%\".format(metrics.accuracy_score(y_test, y_pred_test) * 100))\n",
    "#     print(\"Kappa na skupu za testiranje: {:.2f}\".format(metrics.cohen_kappa_score(y_test, y_pred_test)))\n",
    "#     print(\"Točnost na skupu za treniranje: {:.2f}%\".format(metrics.accuracy_score(y_train, y_pred_train) * 100))\n",
    "#     print(\"Kappa na skupu za treniranje: {:.2f}\".format(metrics.cohen_kappa_score(y_train, y_pred_train)))\n",
    "#     print('\\n\\nTestiranje\\n'+'-'*50)\n",
    "#     print(metrics.classification_report(y_test, y_pred_test, digits=4))\n",
    "#     print('\\n\\nTreniranje\\n'+'-'*50)\n",
    "#     print(metrics.classification_report(y_train, y_pred_train, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluacija_train_test(klasifikator, X_test, y_test, X_train, y_train, NN='False'):\n",
    "    y_pred_test = klasifikator.predict(X_test)\n",
    "    y_train_test = klasifikator.predict(X_train)\n",
    "    print(\"Točnost: {:.2f}%\".format(metrics.accuracy_score(y_test, y_pred_test) * 100))\n",
    "    print(\"F1-macro: {:.2f}%\".format(metrics.f1_score(y_test, y_pred_test, average='macro') * 100))\n",
    "    print(\"Kappa coeff: {:.2f}\".format(metrics.cohen_kappa_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluacija_train_test_nn(klasifikator, X_test, y_test, X_train, y_train, NN='False'):\n",
    "    y_pred_test = klasifikator.predict(X_test)\n",
    "    y_train_test = klasifikator.predict(X_train)\n",
    "    y_pred_test = [np.argmax(p) for p in y_pred_test]\n",
    "    y_train_test = [np.argmax(p) for p in y_train_test]\n",
    "    print(\"Točnost: {:.2f}%\".format(metrics.accuracy_score(y_test, y_pred_test) * 100))\n",
    "    print(\"F1-macro: {:.2f}%\".format(metrics.f1_score(y_test, y_pred_test, average='macro') * 100))\n",
    "    print(\"Kappa coeff: {:.2f}\".format(metrics.cohen_kappa_score(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.225705</td>\n",
       "      <td>0.465176</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>0.329690</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170158</td>\n",
       "      <td>1.249240</td>\n",
       "      <td>0.742869</td>\n",
       "      <td>0.093821</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>1.590318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.423850</td>\n",
       "      <td>1.706131</td>\n",
       "      <td>0.045214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.079163</td>\n",
       "      <td>0.261622</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.005193</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>...</td>\n",
       "      <td>3.043980</td>\n",
       "      <td>4.001054</td>\n",
       "      <td>0.227227</td>\n",
       "      <td>1.208562</td>\n",
       "      <td>0.197082</td>\n",
       "      <td>0.236881</td>\n",
       "      <td>0.037661</td>\n",
       "      <td>1.042218</td>\n",
       "      <td>1.448402</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.009986</td>\n",
       "      <td>0.120012</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.135316</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599024</td>\n",
       "      <td>0.659275</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>2.245741</td>\n",
       "      <td>3.017244</td>\n",
       "      <td>0.380681</td>\n",
       "      <td>0.130854</td>\n",
       "      <td>1.554723</td>\n",
       "      <td>0.695489</td>\n",
       "      <td>0.823782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.455237</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.293728</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667236</td>\n",
       "      <td>1.039222</td>\n",
       "      <td>1.151366</td>\n",
       "      <td>0.070031</td>\n",
       "      <td>1.693817</td>\n",
       "      <td>0.411116</td>\n",
       "      <td>0.067401</td>\n",
       "      <td>1.507691</td>\n",
       "      <td>0.353947</td>\n",
       "      <td>0.420933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>0.317848</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158374</td>\n",
       "      <td>0.044098</td>\n",
       "      <td>1.379212</td>\n",
       "      <td>2.276538</td>\n",
       "      <td>0.071653</td>\n",
       "      <td>0.594772</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.035256</td>\n",
       "      <td>1.876791</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.105571</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>...</td>\n",
       "      <td>2.143079</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>1.702790</td>\n",
       "      <td>0.351741</td>\n",
       "      <td>0.276570</td>\n",
       "      <td>0.156552</td>\n",
       "      <td>0.756936</td>\n",
       "      <td>0.682352</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.075028</td>\n",
       "      <td>0.531064</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.004101</td>\n",
       "      <td>0.154848</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136573</td>\n",
       "      <td>0.897757</td>\n",
       "      <td>0.044842</td>\n",
       "      <td>1.417293</td>\n",
       "      <td>1.045580</td>\n",
       "      <td>0.698649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.637211</td>\n",
       "      <td>0.348459</td>\n",
       "      <td>0.718836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.134349</td>\n",
       "      <td>0.049343</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926211</td>\n",
       "      <td>1.254497</td>\n",
       "      <td>0.899701</td>\n",
       "      <td>1.194468</td>\n",
       "      <td>2.902149</td>\n",
       "      <td>0.044585</td>\n",
       "      <td>0.284327</td>\n",
       "      <td>0.998642</td>\n",
       "      <td>0.837083</td>\n",
       "      <td>0.046890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.006002</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.002417</td>\n",
       "      <td>0.048824</td>\n",
       "      <td>0.085758</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.080284</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>...</td>\n",
       "      <td>4.580097</td>\n",
       "      <td>0.125585</td>\n",
       "      <td>0.111898</td>\n",
       "      <td>3.292334</td>\n",
       "      <td>2.544631</td>\n",
       "      <td>0.193223</td>\n",
       "      <td>1.117164</td>\n",
       "      <td>2.618394</td>\n",
       "      <td>0.885258</td>\n",
       "      <td>0.426386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.099611</td>\n",
       "      <td>0.359887</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.025653</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.294892</td>\n",
       "      <td>0.048998</td>\n",
       "      <td>0.044503</td>\n",
       "      <td>0.712675</td>\n",
       "      <td>0.067461</td>\n",
       "      <td>0.231968</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.357940</td>\n",
       "      <td>0.809386</td>\n",
       "      <td>0.001274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1918 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7     \\\n",
       "361   0.000499  0.000845  0.003744  0.004347  0.225705  0.465176  0.000952   \n",
       "1550  0.000245  0.001998  0.004309  0.001670  0.079163  0.261622  0.000391   \n",
       "1246  0.000663  0.002624  0.002485  0.000736  0.009986  0.120012  0.000754   \n",
       "1799  0.000535  0.005346  0.001310  0.000457  0.174312  0.455237  0.000278   \n",
       "427   0.000371  0.003905  0.002765  0.005340  0.124150  0.317848  0.000849   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "835   0.000599  0.004914  0.004246  0.002547  0.062495  0.105571  0.000651   \n",
       "1216  0.000393  0.004505  0.002774  0.004187  0.075028  0.531064  0.000773   \n",
       "1653  0.000388  0.002860  0.002769  0.002725  0.134349  0.049343  0.000555   \n",
       "559   0.000586  0.006002  0.001873  0.002417  0.048824  0.085758  0.000428   \n",
       "684   0.000172  0.001883  0.002868  0.003205  0.099611  0.359887  0.000862   \n",
       "\n",
       "          8         9         10    ...      1015      1016      1017  \\\n",
       "361   0.003710  0.329690  0.000255  ...  0.170158  1.249240  0.742869   \n",
       "1550  0.005193  0.016719  0.000313  ...  3.043980  4.001054  0.227227   \n",
       "1246  0.002546  0.135316  0.000681  ...  0.599024  0.659275  0.043686   \n",
       "1799  0.002833  0.293728  0.000331  ...  0.667236  1.039222  1.151366   \n",
       "427   0.005082  0.035795  0.000264  ...  0.158374  0.044098  1.379212   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "835   0.003279  0.002084  0.000282  ...  2.143079  0.000334  0.075804   \n",
       "1216  0.004101  0.154848  0.000353  ...  0.136573  0.897757  0.044842   \n",
       "1653  0.005085  0.019213  0.000342  ...  0.926211  1.254497  0.899701   \n",
       "559   0.005277  0.080284  0.000472  ...  4.580097  0.125585  0.111898   \n",
       "684   0.005966  0.025653  0.000188  ...  1.294892  0.048998  0.044503   \n",
       "\n",
       "          1018      1019      1020      1021      1022      1023      1024  \n",
       "361   0.093821  0.001713  1.590318  0.000000  3.423850  1.706131  0.045214  \n",
       "1550  1.208562  0.197082  0.236881  0.037661  1.042218  1.448402  0.000000  \n",
       "1246  2.245741  3.017244  0.380681  0.130854  1.554723  0.695489  0.823782  \n",
       "1799  0.070031  1.693817  0.411116  0.067401  1.507691  0.353947  0.420933  \n",
       "427   2.276538  0.071653  0.594772  0.000246  0.035256  1.876791  0.000000  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "835   1.702790  0.351741  0.276570  0.156552  0.756936  0.682352  0.000000  \n",
       "1216  1.417293  1.045580  0.698649  0.000000  3.637211  0.348459  0.718836  \n",
       "1653  1.194468  2.902149  0.044585  0.284327  0.998642  0.837083  0.046890  \n",
       "559   3.292334  2.544631  0.193223  1.117164  2.618394  0.885258  0.426386  \n",
       "684   0.712675  0.067461  0.231968  0.010724  0.357940  0.809386  0.001274  \n",
       "\n",
       "[1918 rows x 1024 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "lb = LabelBinarizer()\n",
    "le.fit(y_train)\n",
    "\n",
    "labele_train = le.transform(y_train)\n",
    "# np.unique(labele_train)\n",
    "one_hot_train = lb.fit_transform(labele_train.reshape(-1,1))\n",
    "one_hot_train\n",
    "\n",
    "labele_test = le.transform(y_test)\n",
    "one_hot_test = lb.fit_transform(labele_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(Dense(256, activation ='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(6,activation='softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=LR), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  774       \n",
      "=================================================================\n",
      "Total params: 297,606\n",
      "Trainable params: 296,838\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1918 samples, validate on 477 samples\n",
      "Epoch 1/100\n",
      "1918/1918 [==============================] - 1s 695us/sample - loss: 1.2342 - accuracy: 0.5615 - val_loss: 0.8544 - val_accuracy: 0.7233\n",
      "Epoch 2/100\n",
      "1918/1918 [==============================] - 0s 196us/sample - loss: 0.6411 - accuracy: 0.7826 - val_loss: 0.7525 - val_accuracy: 0.7338\n",
      "Epoch 3/100\n",
      "1918/1918 [==============================] - 0s 199us/sample - loss: 0.4523 - accuracy: 0.8457 - val_loss: 0.6657 - val_accuracy: 0.7736\n",
      "Epoch 4/100\n",
      "1918/1918 [==============================] - 0s 204us/sample - loss: 0.3493 - accuracy: 0.8858 - val_loss: 0.6681 - val_accuracy: 0.7799\n",
      "Epoch 5/100\n",
      "1918/1918 [==============================] - 0s 202us/sample - loss: 0.2788 - accuracy: 0.9161 - val_loss: 0.6991 - val_accuracy: 0.7820\n",
      "Epoch 6/100\n",
      "1918/1918 [==============================] - 0s 199us/sample - loss: 0.2175 - accuracy: 0.9364 - val_loss: 0.6825 - val_accuracy: 0.7715\n",
      "Epoch 7/100\n",
      "1918/1918 [==============================] - 0s 207us/sample - loss: 0.2075 - accuracy: 0.9364 - val_loss: 0.6814 - val_accuracy: 0.7904\n",
      "Epoch 8/100\n",
      "1918/1918 [==============================] - 0s 199us/sample - loss: 0.1646 - accuracy: 0.9531 - val_loss: 0.7437 - val_accuracy: 0.7757\n",
      "Epoch 9/100\n",
      "1918/1918 [==============================] - 0s 195us/sample - loss: 0.1260 - accuracy: 0.9724 - val_loss: 0.8288 - val_accuracy: 0.7757\n",
      "Epoch 10/100\n",
      "1918/1918 [==============================] - 0s 193us/sample - loss: 0.1181 - accuracy: 0.9651 - val_loss: 0.7476 - val_accuracy: 0.7904\n",
      "Epoch 11/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.1110 - accuracy: 0.9713 - val_loss: 0.8338 - val_accuracy: 0.7757\n",
      "Epoch 12/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.1177 - accuracy: 0.9630 - val_loss: 0.8046 - val_accuracy: 0.7652\n",
      "Epoch 13/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0927 - accuracy: 0.9745 - val_loss: 0.8497 - val_accuracy: 0.7568\n",
      "Epoch 14/100\n",
      "1918/1918 [==============================] - 0s 182us/sample - loss: 0.0899 - accuracy: 0.9724 - val_loss: 0.8949 - val_accuracy: 0.7778\n",
      "Epoch 15/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0918 - accuracy: 0.9755 - val_loss: 0.8335 - val_accuracy: 0.7820\n",
      "Epoch 16/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.1027 - accuracy: 0.9687 - val_loss: 0.8583 - val_accuracy: 0.7715\n",
      "Epoch 17/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0761 - accuracy: 0.9786 - val_loss: 0.7941 - val_accuracy: 0.7736\n",
      "Epoch 18/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0905 - accuracy: 0.9745 - val_loss: 1.0838 - val_accuracy: 0.7338\n",
      "Epoch 19/100\n",
      "1918/1918 [==============================] - 0s 188us/sample - loss: 0.0737 - accuracy: 0.9791 - val_loss: 0.8908 - val_accuracy: 0.7904\n",
      "Epoch 20/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0844 - accuracy: 0.9765 - val_loss: 1.0375 - val_accuracy: 0.7400\n",
      "Epoch 21/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.0687 - accuracy: 0.9791 - val_loss: 0.8390 - val_accuracy: 0.7820\n",
      "Epoch 22/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0584 - accuracy: 0.9828 - val_loss: 1.1701 - val_accuracy: 0.7589\n",
      "Epoch 23/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0677 - accuracy: 0.9791 - val_loss: 0.9990 - val_accuracy: 0.7694\n",
      "Epoch 24/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0515 - accuracy: 0.9875 - val_loss: 0.9203 - val_accuracy: 0.7463\n",
      "Epoch 25/100\n",
      "1918/1918 [==============================] - 0s 172us/sample - loss: 0.0553 - accuracy: 0.9849 - val_loss: 0.9351 - val_accuracy: 0.7568\n",
      "Epoch 26/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0565 - accuracy: 0.9854 - val_loss: 0.9632 - val_accuracy: 0.7841\n",
      "Epoch 27/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0636 - accuracy: 0.9791 - val_loss: 1.2781 - val_accuracy: 0.7275\n",
      "Epoch 28/100\n",
      "1918/1918 [==============================] - 0s 183us/sample - loss: 0.0922 - accuracy: 0.9666 - val_loss: 1.0740 - val_accuracy: 0.7547\n",
      "Epoch 29/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.1338 - accuracy: 0.9520 - val_loss: 1.1584 - val_accuracy: 0.7421\n",
      "Epoch 30/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0965 - accuracy: 0.9713 - val_loss: 0.9998 - val_accuracy: 0.7568\n",
      "Epoch 31/100\n",
      "1918/1918 [==============================] - 0s 182us/sample - loss: 0.0690 - accuracy: 0.9776 - val_loss: 0.9545 - val_accuracy: 0.7673\n",
      "Epoch 32/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0553 - accuracy: 0.9823 - val_loss: 1.0753 - val_accuracy: 0.7484\n",
      "Epoch 33/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0645 - accuracy: 0.9807 - val_loss: 1.1439 - val_accuracy: 0.7296\n",
      "Epoch 34/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0758 - accuracy: 0.9765 - val_loss: 0.8966 - val_accuracy: 0.7778\n",
      "Epoch 35/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.0577 - accuracy: 0.9791 - val_loss: 1.0093 - val_accuracy: 0.7694\n",
      "Epoch 36/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0516 - accuracy: 0.9838 - val_loss: 0.9664 - val_accuracy: 0.7568\n",
      "Epoch 37/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0508 - accuracy: 0.9838 - val_loss: 1.1920 - val_accuracy: 0.7358\n",
      "Epoch 38/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0460 - accuracy: 0.9880 - val_loss: 1.0010 - val_accuracy: 0.7820\n",
      "Epoch 39/100\n",
      "1918/1918 [==============================] - 0s 173us/sample - loss: 0.0360 - accuracy: 0.9922 - val_loss: 1.1216 - val_accuracy: 0.7652\n",
      "Epoch 40/100\n",
      "1918/1918 [==============================] - 0s 184us/sample - loss: 0.0523 - accuracy: 0.9838 - val_loss: 1.0535 - val_accuracy: 0.7694\n",
      "Epoch 41/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0391 - accuracy: 0.9901 - val_loss: 1.1146 - val_accuracy: 0.7568\n",
      "Epoch 42/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0431 - accuracy: 0.9885 - val_loss: 1.2738 - val_accuracy: 0.7568\n",
      "Epoch 43/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0650 - accuracy: 0.9760 - val_loss: 1.0960 - val_accuracy: 0.7631\n",
      "Epoch 44/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0582 - accuracy: 0.9828 - val_loss: 1.3178 - val_accuracy: 0.7400\n",
      "Epoch 45/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0577 - accuracy: 0.9823 - val_loss: 1.0423 - val_accuracy: 0.7778\n",
      "Epoch 46/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0480 - accuracy: 0.9859 - val_loss: 1.1420 - val_accuracy: 0.7589\n",
      "Epoch 47/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0496 - accuracy: 0.9854 - val_loss: 1.1678 - val_accuracy: 0.7379\n",
      "Epoch 48/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0457 - accuracy: 0.9828 - val_loss: 1.1748 - val_accuracy: 0.7379\n",
      "Epoch 49/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0435 - accuracy: 0.9875 - val_loss: 1.2116 - val_accuracy: 0.7379\n",
      "Epoch 50/100\n",
      "1918/1918 [==============================] - 0s 183us/sample - loss: 0.0216 - accuracy: 0.9943 - val_loss: 1.0664 - val_accuracy: 0.7610\n",
      "Epoch 51/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0187 - accuracy: 0.9943 - val_loss: 1.0694 - val_accuracy: 0.7610\n",
      "Epoch 52/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0489 - accuracy: 0.9828 - val_loss: 1.1406 - val_accuracy: 0.7631\n",
      "Epoch 53/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0522 - accuracy: 0.9859 - val_loss: 1.1690 - val_accuracy: 0.7694\n",
      "Epoch 54/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.0361 - accuracy: 0.9906 - val_loss: 1.3332 - val_accuracy: 0.7338\n",
      "Epoch 55/100\n",
      "1918/1918 [==============================] - 0s 183us/sample - loss: 0.0426 - accuracy: 0.9854 - val_loss: 1.1892 - val_accuracy: 0.7673\n",
      "Epoch 56/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0407 - accuracy: 0.9880 - val_loss: 1.1295 - val_accuracy: 0.7841\n",
      "Epoch 57/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0396 - accuracy: 0.9864 - val_loss: 1.2312 - val_accuracy: 0.7568\n",
      "Epoch 58/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0256 - accuracy: 0.9922 - val_loss: 1.2363 - val_accuracy: 0.7673\n",
      "Epoch 59/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0208 - accuracy: 0.9958 - val_loss: 1.1462 - val_accuracy: 0.7673\n",
      "Epoch 60/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0284 - accuracy: 0.9937 - val_loss: 1.1884 - val_accuracy: 0.7547\n",
      "Epoch 61/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0290 - accuracy: 0.9911 - val_loss: 1.1867 - val_accuracy: 0.7652\n",
      "Epoch 62/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0349 - accuracy: 0.9911 - val_loss: 1.3121 - val_accuracy: 0.7652\n",
      "Epoch 63/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0426 - accuracy: 0.9870 - val_loss: 1.2800 - val_accuracy: 0.7610\n",
      "Epoch 64/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0254 - accuracy: 0.9943 - val_loss: 1.1460 - val_accuracy: 0.7925\n",
      "Epoch 65/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.0196 - accuracy: 0.9937 - val_loss: 1.1715 - val_accuracy: 0.7799\n",
      "Epoch 66/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0218 - accuracy: 0.9911 - val_loss: 1.2256 - val_accuracy: 0.7715\n",
      "Epoch 67/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0499 - accuracy: 0.9859 - val_loss: 1.2728 - val_accuracy: 0.7673\n",
      "Epoch 68/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0300 - accuracy: 0.9901 - val_loss: 1.1373 - val_accuracy: 0.7463\n",
      "Epoch 69/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0358 - accuracy: 0.9917 - val_loss: 1.2002 - val_accuracy: 0.7589\n",
      "Epoch 70/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0294 - accuracy: 0.9891 - val_loss: 1.2793 - val_accuracy: 0.7463\n",
      "Epoch 71/100\n",
      "1918/1918 [==============================] - 0s 183us/sample - loss: 0.0313 - accuracy: 0.9917 - val_loss: 1.2985 - val_accuracy: 0.7694\n",
      "Epoch 72/100\n",
      "1918/1918 [==============================] - 0s 182us/sample - loss: 0.0287 - accuracy: 0.9901 - val_loss: 1.1390 - val_accuracy: 0.7757\n",
      "Epoch 73/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0388 - accuracy: 0.9880 - val_loss: 1.1770 - val_accuracy: 0.7778\n",
      "Epoch 74/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0389 - accuracy: 0.9849 - val_loss: 1.1902 - val_accuracy: 0.7694\n",
      "Epoch 75/100\n",
      "1918/1918 [==============================] - 0s 183us/sample - loss: 0.0322 - accuracy: 0.9901 - val_loss: 1.1526 - val_accuracy: 0.7757\n",
      "Epoch 76/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.0389 - accuracy: 0.9880 - val_loss: 1.2730 - val_accuracy: 0.7568\n",
      "Epoch 77/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0305 - accuracy: 0.9880 - val_loss: 1.2180 - val_accuracy: 0.7673\n",
      "Epoch 78/100\n",
      "1918/1918 [==============================] - 0s 175us/sample - loss: 0.0316 - accuracy: 0.9917 - val_loss: 1.1686 - val_accuracy: 0.7757\n",
      "Epoch 79/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0265 - accuracy: 0.9922 - val_loss: 1.2073 - val_accuracy: 0.7778\n",
      "Epoch 80/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0207 - accuracy: 0.9948 - val_loss: 1.0155 - val_accuracy: 0.7966\n",
      "Epoch 81/100\n",
      "1918/1918 [==============================] - 0s 177us/sample - loss: 0.0159 - accuracy: 0.9953 - val_loss: 1.0696 - val_accuracy: 0.7757\n",
      "Epoch 82/100\n",
      "1918/1918 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.99 - 0s 182us/sample - loss: 0.0167 - accuracy: 0.9943 - val_loss: 1.0669 - val_accuracy: 0.7757\n",
      "Epoch 83/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0173 - accuracy: 0.9948 - val_loss: 1.0756 - val_accuracy: 0.7862\n",
      "Epoch 84/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0123 - accuracy: 0.9974 - val_loss: 1.1784 - val_accuracy: 0.7945\n",
      "Epoch 85/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0317 - accuracy: 0.9891 - val_loss: 1.7218 - val_accuracy: 0.7275\n",
      "Epoch 86/100\n",
      "1918/1918 [==============================] - 0s 174us/sample - loss: 0.0253 - accuracy: 0.9943 - val_loss: 1.2483 - val_accuracy: 0.7778\n",
      "Epoch 87/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0303 - accuracy: 0.9917 - val_loss: 1.1783 - val_accuracy: 0.7862\n",
      "Epoch 88/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0261 - accuracy: 0.9911 - val_loss: 1.2607 - val_accuracy: 0.7673\n",
      "Epoch 89/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0486 - accuracy: 0.9849 - val_loss: 1.2638 - val_accuracy: 0.7484\n",
      "Epoch 90/100\n",
      "1918/1918 [==============================] - 0s 182us/sample - loss: 0.0411 - accuracy: 0.9896 - val_loss: 1.1771 - val_accuracy: 0.7799\n",
      "Epoch 91/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0285 - accuracy: 0.9922 - val_loss: 1.3865 - val_accuracy: 0.7547\n",
      "Epoch 92/100\n",
      "1918/1918 [==============================] - 0s 179us/sample - loss: 0.0334 - accuracy: 0.9891 - val_loss: 1.2180 - val_accuracy: 0.7589\n",
      "Epoch 93/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0224 - accuracy: 0.9943 - val_loss: 1.3037 - val_accuracy: 0.7673\n",
      "Epoch 94/100\n",
      "1918/1918 [==============================] - 0s 178us/sample - loss: 0.0247 - accuracy: 0.9917 - val_loss: 1.1307 - val_accuracy: 0.7820\n",
      "Epoch 95/100\n",
      "1918/1918 [==============================] - 0s 181us/sample - loss: 0.0305 - accuracy: 0.9906 - val_loss: 1.1801 - val_accuracy: 0.7505\n",
      "Epoch 96/100\n",
      "1918/1918 [==============================] - 0s 176us/sample - loss: 0.0267 - accuracy: 0.9917 - val_loss: 1.2089 - val_accuracy: 0.7694\n",
      "Epoch 97/100\n",
      "1918/1918 [==============================] - 0s 182us/sample - loss: 0.0352 - accuracy: 0.9891 - val_loss: 1.1895 - val_accuracy: 0.7547\n",
      "Epoch 98/100\n",
      "1918/1918 [==============================] - 0s 180us/sample - loss: 0.0414 - accuracy: 0.9880 - val_loss: 1.1363 - val_accuracy: 0.7652\n",
      "Epoch 99/100\n",
      "1918/1918 [==============================] - 0s 186us/sample - loss: 0.0293 - accuracy: 0.9906 - val_loss: 1.2356 - val_accuracy: 0.7652\n",
      "Epoch 100/100\n",
      "1918/1918 [==============================] - 0s 182us/sample - loss: 0.0274 - accuracy: 0.9885 - val_loss: 1.1624 - val_accuracy: 0.7673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2124ca851d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_NN = np.array(X_train)\n",
    "X_test_NN = np.array(X_test)\n",
    "model.fit(X_train_NN, labele_train, batch_size=16, epochs=100, \n",
    "          validation_data=(X_test_NN, labele_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODELI + 'DenseNet121_FE.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Točnost: 76.73%\n",
      "F1-macro: 76.60%\n",
      "Kappa coeff: 0.71\n"
     ]
    }
   ],
   "source": [
    "evaluacija_train_test_nn(model, X_test_NN, le.transform(y_test), X_train_NN , le.transform(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Točnost: 71.91%\n",
      "F1-macro: 68.40%\n",
      "Kappa coeff: 0.64\n"
     ]
    }
   ],
   "source": [
    "evaluacija_train_test(rf, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1          0.000031\n",
       "2          0.003464\n",
       "3          0.001855\n",
       "4          0.001270\n",
       "5          2.571563\n",
       "           ...     \n",
       "1020     803.999116\n",
       "1021     103.636925\n",
       "1022    3216.817887\n",
       "1023     597.609260\n",
       "1024     269.717453\n",
       "Length: 1024, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.var()*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Točnost: 79.04%\n",
      "F1-macro: 78.85%\n",
      "Kappa coeff: 0.74\n"
     ]
    }
   ],
   "source": [
    "evaluacija_train_test(clf, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Točnost: 64.57%\n",
      "F1-macro: 65.43%\n",
      "Kappa coeff: 0.56\n"
     ]
    }
   ],
   "source": [
    "evaluacija_train_test(gnb, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antem\\Anaconda3\\envs\\ivana_dl\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Točnost: 74.63%\n",
      "F1-macro: 76.04%\n",
      "Kappa coeff: 0.68\n"
     ]
    }
   ],
   "source": [
    "evaluacija_train_test(lr, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Točnost: 70.65%\n",
      "F1-macro: 70.18%\n",
      "Kappa coeff: 0.63\n"
     ]
    }
   ],
   "source": [
    "evaluacija_train_test(knn, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
